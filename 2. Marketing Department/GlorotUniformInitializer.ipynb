{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7486a1",
   "metadata": {},
   "source": [
    "## 🎯 Glorot Uniform Initializer Nedir?\n",
    "\n",
    "> **Glorot uniform initializer**, sinir ağı ağırlıklarının başlangıçta hangi değerlere atanacağını belirleyen bir yöntemdir.\n",
    "\n",
    "Yani bu initializer, **her nöronun ağırlıklarını rastgele ama kontrollü** bir şekilde başlatır. Ama öyle rastgele değil — **istatistiksel zekâyla** rastgele. Çünkü yanlış bir başlangıç, modelin öğrenmesini **ya yavaşlatır ya da tamamen mahveder**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Neden İhtiyacımız Var?\n",
    "\n",
    "Eğer ağırlıkları:\n",
    "- **Çok küçük başlatırsak**: Gradient'ler yok olur (→ **vanishing gradient**),\n",
    "- **Çok büyük başlatırsak**: Gradient'ler patlar (→ **exploding gradient**).\n",
    "\n",
    "İşte tam bu noktada Glorot devreye giriyor:\n",
    "> Ağırlıkları öyle bir aralıkta başlat ki, hem **forward pass** (aktivasyonlar), hem de **backward pass** (gradientler) katmanlar arasında istikrarlı kalsın.\n",
    "\n",
    "---\n",
    "\n",
    "## 📜 Glorot'un Formülü:\n",
    "\n",
    "Glorot uniform şunu der:\n",
    "```python\n",
    "limit = sqrt(6 / (fan_in + fan_out))\n",
    "```\n",
    "\n",
    "Sonra ağırlıklar şu aralıktan rastgele seçilir:\n",
    "```python\n",
    "[-limit, +limit]\n",
    "```\n",
    "\n",
    "- `fan_in` → Bu katmana giren nöron sayısı\n",
    "- `fan_out` → Bu katmandan çıkan nöron sayısı\n",
    "\n",
    "> 🎓 Bu formül, Glorot & Bengio’nun 2010’daki makalesinden geliyor:  \n",
    "> _\"Understanding the difficulty of training deep feedforward neural networks\"_\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Autoencoder'da Ne İşe Yarar?\n",
    "\n",
    "Autoencoder'larda encoder ve decoder arasında veri \"sıkışır\" → hassas ve dengeli bir öğrenme gerekir.  \n",
    "İşte burada **Glorot uniform** harika çalışır çünkü:\n",
    "\n",
    "### 🧬 1. **Veri akışını dengeler (forward pass)**  \n",
    "- Encoder'dan latent'e, oradan decoder'a geçerken sinyallerin bozulmamasını sağlar.\n",
    "\n",
    "### 🧠 2. **Backpropagation sırasında gradientlerin dengesini korur**  \n",
    "- Böylece model düzgün öğrenir. Ne çok yavaş, ne de patlayarak.\n",
    "\n",
    "### 🔁 3. **Sembolikte “simetrik” bir yapı olduğu için daha da kritik**\n",
    "- Encoder ve decoder simetrik yapılar → aynı initializer kullanmak, öğrenmeyi daha kararlı kılar.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Keras’ta Nasıl Kullanılır?\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "encoded = Dense(32, activation='relu', kernel_initializer=GlorotUniform())(input_layer)\n",
    "```\n",
    "\n",
    "Not: Keras’ta zaten varsayılan initializer genelde `glorot_uniform`, yani belirtmesen bile çoğu zaman zaten bunu kullanıyor. Ama bilinçli kullanmak iyidir 💡\n",
    "\n",
    "---\n",
    "\n",
    "## ⚔️ Glorot vs Diğer Initializer’lar\n",
    "\n",
    "| Initializer       | Kullanım Yeri                         |\n",
    "|-------------------|----------------------------------------|\n",
    "| **Glorot Uniform** | Genellikle ReLU dışındaki aktivasyonlar (tanh, sigmoid) |\n",
    "| **He Initialization** | ReLU ve türevleri (LeakyReLU, ELU) için daha iyi |\n",
    "| **LeCun**         | Sigmoid/selu gibi özel durumlar        |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎁 Kapanış Notu:\n",
    "\n",
    "- Autoencoder gibi derin yapılarda, ağırlıkların doğru başlatılması **öğrenmenin kaderini belirler.**\n",
    "- Glorot, bu işin matematiksel temelli, dengeli ve güvenilir yöntemidir.\n",
    "- Öğrenme hızını artırır, modelin daha sağlıklı konverge etmesini sağlar."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
